# VivoAssist â€“ Manual-Aware RAG System ğŸ“˜ğŸ¤–

VivoAssist is a **Retrieval-Augmented Generation (RAG)** system built to answer user questions **strictly from PDF manuals** with **page-level citations**, while preventing hallucinations and cross-manual leakage.

It is designed to be **generic and production-ready**, initially tested with vehicle manuals and later intended for **telecom product manuals**.

---

## âœ¨ Key Features

- ğŸ“„ Ingests **multiple large PDF manuals**
- ğŸ§© **Hierarchical chunking** (big / mid / small)
- ğŸ§  Vector search using **ChromaDB (persistent)**
- ğŸ”’ **Manual-aware retrieval** (prevents cross-manual answers)
- ğŸ§  Context-aware follow-up questions (sticky manual scope)
- ğŸ“‘ **Page-level citations** in answers
- ğŸš« Strict guard:
  If content is not in the selected manual â†’
  **â€œNot found in the manual.â€**
- âš¡ Azure OpenAI powered (GPT-4o + embeddings)

---

## ğŸ—ï¸ Architecture Overview

```
PDF Manuals
   â†“
PDF Loader (page-level docs)
   â†“
Hierarchical Chunking
(big / mid / small)
   â†“
ChromaDB (persistent vectors)
   â†“
LlamaIndex VectorStoreIndex
   â†“
Chat Engine (manual-aware)
```

---

## ğŸ“‚ Project Structure

```
app/
 â”œâ”€ chat/
 â”‚   â””â”€ chat_engine.py        # Terminal chat with strict manual rules
 â”œâ”€ config/
 â”‚   â””â”€ settings.py           # App config + Azure OpenAI setup
 â”œâ”€ ingestion/
 â”‚   â”œâ”€ pdf_loader.py         # Page-wise PDF loading
 |   â”œâ”€ diagram_extractor.py
 â”‚   â””â”€ chunker.py            # Hierarchical chunking logic
 â”œâ”€ index/
 â”‚   â”œâ”€ chroma_store.py       # Persistent Chroma DB
 â”‚   â””â”€ index_builder.py      # Index build + throttling
 â”œâ”€ utils/
 â”‚   â”œâ”€ debug.py              # Chunk + retrieval debugging
 |   â”œâ”€ manual_registry.py
 |   â”œâ”€ manual_selector.py
 |   â””â”€ models_registry.py
 â””â”€ main.py                   # Entry point (CLI)
data/
 â””â”€ manuals/                  # PDF manuals
chroma_db/                    # Persistent vector store
.env                           # Azure credentials
requirements.txt
.gitignore
```

---

## ğŸ§  Chunking Strategy (Hierarchical)

Each PDF is split into **three levels of chunks**:

| Level | Purpose              |
| ----- | -------------------- |
| Big   | High-level context   |
| Mid   | Section-level detail |
| Small | Precise answers      |

All chunks contain metadata:

- `file_name`
- `page_number / page_label`
- `chunk_level`

This allows:

- Better recall
- Accurate citations
- Reduced hallucinations

---

## ğŸš€ Setup & Installation

### 1ï¸âƒ£ Create virtual environment

```bash
py -V:3.10 -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment variables

Create a `.env` file:

```env
AZURE_OPENAI_ENDPOINT=your_endpoint
AZURE_OPENAI_API_KEY=your_api_key
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o
AZURE_OPENAI_EMBED_DEPLOYMENT=text-embedding-3-large
```

âš ï¸ **Never commit `.env` to GitHub**

---

## â–¶ï¸ Running the App

### First run (build index)

```bash
python -m app.main
```

### Rebuild index (when manuals change)

```bash
python -m app.main --rebuild-index
```

---

## ğŸ’¬ Chat Rules (Very Important)

The assistant **WILL ONLY**:

- Answer using the selected PDF manual
- Use retrieved chunks as sources
- Show page numbers when available

If information is missing:

```
Not found in the manual.
```

No guessing. No external knowledge.

## ğŸ› ï¸ Debug Mode (Optional)

Enable debug in `settings.py`:

```python
debug = True
```

Youâ€™ll get:

- Chunk counts
- Sample chunk previews
- Retrieval score breakdowns

---

## ğŸ¯ Future Improvements

- Image extraction from manuals
- Diagram-based grounding
- Better manual auto-selection
- Web UI (instead of terminal)
- Telco-scale manuals (1000+ pages)

---

